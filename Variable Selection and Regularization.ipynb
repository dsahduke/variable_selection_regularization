{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far, all of the methods that we have described assume that we have some data, $\\textbf{X}$. Our goal has been to learn a mapping from the data, $\\textbf{X}$, to some labels $\\textbf{y}$. However, what if some elements of $\\textbf{X}$ are not useful in this mapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ## Variable Selection\n",
    "Given some predictors $x_1, x_2, \\dots$, how do we select the subset that are most useful in our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this example, let's say we have $p$ predictors. That is, our predictors are $x_j, j = 1 \\dots p$. How do we determine which indices $j$ should belong in a final model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The naive way to do this is to just fit every single possible model and then select the best one according to some criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, we would fit every model:\n",
    "\n",
    "$$ y = \\beta_0 $$\n",
    "$$ y = \\beta_0 + \\beta_1x_1 $$\n",
    "$$ y = \\beta_0 + \\beta_1x_2 $$ \n",
    "$$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This will result in $2^p$ models, which is ... a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Practically, this subset selection is done in stages, that is:\n",
    "\n",
    " 1. for $ j = 1 \\dots p $:\n",
    "     - Fit all ${p \\choose j}$ possible models that have $j$ predictors\n",
    "     - Pick the best model among those according to the smallest RSS or largest $R^2$\n",
    " 2. for $ j = 1 \\dots p $ and the null model, $y = \\beta_0$, select the model among the \"best\" models according to some criterion ($C_p$, BIC, adjusted $R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem with this method is obvious -- with $p$ = 20, there are over 1 million possible models to consider. Consider the problem of multiple comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we narrow down the number of models to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stepwise Selection\n",
    "\n",
    "ISL - http://www-bcf.usc.edu/~gareth/ISL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead of choosing from all $2^p$ models, we start with a model with no predictors, and then add predictors one at a time to see if it helps the model until either all the predictors are added, or there is no additional benefit to adding a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Start with the simple model: $y = \\beta_0$\n",
    "\n",
    "2. for $ j = 0 \\dots, p-1 $:\n",
    "    - Fit all $p - j$ models that add 1 predictor to the current model\n",
    "    - Select the best among these\n",
    "        + This can be done by looking at minimizing RSS, or looking at $R^2$\n",
    "3. Select the best model according to $C_p$, BIC, or adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Start with the simple model: $y = \\beta_0$\n",
    "\n",
    "2. for $ j = 0 \\dots, p-1 $:\n",
    "    - Fit all $p - j$ models that add 1 predictor to the current model\n",
    "    - Select the best among these\n",
    "        + Run a F-test to check for model significance. Pick the model with the highest $p-valu$\n",
    "3. Continue until the F-test is no longer significant at the desired $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These methods are greedy -- they consider only a small subset of the possible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exactly the opposite procedure of Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Start with the full model: $ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 \\dots \\beta_px_p $\n",
    "2. for $j = p, p-1 \\dots 1$:\n",
    "    - Consider all $j$ models that contain every predictor except 1\n",
    "    - Choose the best model out of these\n",
    "3. Select the best model according to $C_p$, BIC, or adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These two approaches may arrive at different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forward-Backward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This procedure, which was inspired by the forward stepwise selection and backward stepwise selection procedure, works as follows:\n",
    "\n",
    "Define $\\alpha_1$ to be the significance cutoff to add a variable and $\\alpha_2$ to be the significance level for removing a variable.\n",
    "\n",
    "1. Start with the null model: $y = \\beta_0$\n",
    "2. Perform 1 step of the Forward Stepwise Selection Procedure, looking at the the F-test p-value to determine which variable to add.\n",
    "3. After each Forward Stepwise Selection Procedure step, check the significance of each of the other predictors in the model, and remove them if they fall below $\\alpha_2$.\n",
    "4. Continue until all variables are in the model, or Forward Stepwise Selection Procedure falls above $\\alpha_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing a best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that $R^2$, which is a function of RSS, always increases when you add predictors. Therefore, it is not a good measure for which model is best. \n",
    "\n",
    "RSS = Residual Sum of Squares = $\\sum_{i=1}^{m} (y_i - f(\\boldsymbol{x_i}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### $C_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If a model has $d$ predictors, $C_p$ estimates the *test* MSE:\n",
    "\n",
    "$$ C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\sigma^2$ is an estimate of the variance of the error $\\epsilon_i$. (For linear regression, the full form is $y = \\beta_0 + \\beta_1x_1 \\dots \\beta_dx_d + \\epsilon_i, \\epsilon_i \\sim N(0, \\sigma^2)$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice how this goes up as the the number of predictors increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Information Criterion (BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The BIC looks like $C_p$, but is derived from the bayesian point of view.\n",
    "\n",
    "It is given by:\n",
    "\n",
    "$$ BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It replaces the factor of 2 in the $C_p$ with log(n). However, for any n > 7, this will end up being >2. Therefore, the BIC is often said to result in smaller models than $C_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adjusted $R^2$\n",
    "\n",
    "Recall that $R^2 = RSS/TSS$\n",
    "\n",
    "OR\n",
    "\n",
    "$$R^2 = \\frac{var(\\boldsymbol{\\beta}^T\\textbf{x})}{var(y)} = \\frac{\\Sigma_{i=1}^n(\\hat{y}_i - \\bar{y})^2}{\\Sigma_{i=1}^n(y_i - \\bar{y})} $$\n",
    "\n",
    "The Adjusted $R^2$ is a method to use $R^2$ to select models. As we have seen, $R^2$ tends to increase when you add variables to a model. Therfore, adjusted $R^2$ penalizes the number of parameters:\n",
    "\n",
    "$$Adjusted R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In linear regression, we try to minimize the mean square error, which in general is given by\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2 $$ where $\\hat{f}(x_i)$ is your model prediction.\n",
    "\n",
    "Let's say the true model for Y is given by $Y = f(x)$\n",
    "\n",
    "We can define a bias as being $E[\\hat{f}(x)] - f(x)$\n",
    "\n",
    "Then, we can decompose the expression as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ E[(\\hat{f}(x) - f(x))^2] = MSE$$\n",
    "\n",
    "define $\\mu = E[\\hat{f}(x)]$\n",
    "\n",
    "$$ E[(\\hat{f}(x) - f(x))^2] = E[(\\hat{f}(x) - \\mu) + (\\mu - f(x))^2] $$\n",
    "\n",
    "$$ = E[(\\hat{f}(x) - \\mu)^2 + 2(\\hat{f}(x) - \\mu)(\\mu - f(x)) + (\\mu-f(x))^2] $$\n",
    "\n",
    "Note that $E[\\hat{f}(x) - \\mu] = E[\\hat{f}(x) - E[\\hat{f}(x)]] = 0 $\n",
    "\n",
    "$$ E[(\\hat{f}(x) - f(x))^2] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + (E[\\hat{f}(x)] - f(x))^2 $$\n",
    "$$ = var(\\hat{f}(x)) + bias(\\hat{f}(x))^2 $$\n",
    "\n",
    "*note:* Here, we've ommitted another term which is the *irreducible* error from $Var(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is $var(\\hat{f}(x))$? What is the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The *variance* refers to how much $\\hat{f}(x)$ changes if it were estimated from a different training set. Ideally, this variance would be small, since small changes in the dataset should not produce a large change in the model estimate. \n",
    "\n",
    "In general, more flexible methods have higher variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The *bias* arises because we are often trying to model a complex phenomenon, but the model is too simplistic, or does not have enough information to perfectly model the phenomenon. Does a linear regression actually capture all of the intracacies and nonlinear patterns in the data?\n",
    "\n",
    "Generally, more flexible methods tend to have lower bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, as the methods become more flexible (more parameters, etc.), the variance will increase and the bias will decrease. This is where the trade-off comes from. The goal of model selection should be to find the right mix of bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You could draw a line through every point (low bias, high variance) or draw just a horizontal line (high bias, low variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./assets/biasvariance.png)\n",
    "http://www-bcf.usc.edu/~gareth/ISL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shrinkage methods and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than explicitly selecting variables, we can *constrain* or *regularize* the coefficient estimates. These are known as *shrinkage* methods because they *shrink* the coefficient estimates towards 0. The reason why this might be a good idea is that it can often dramatically reduce the variance in the estimates, which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The two most popular methods for regularization are known as Ridge Regression and LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that in linear regression, we are interested in minimizing the squared-error loss, or maximizing the log likelihood. \n",
    "\n",
    "$$ \\hat{\\beta} = \\underset{\\beta}{\\arg\\min} \\sum_{i=1}^{n}(y_i - \\boldsymbol{\\beta}^T\\boldsymbol{x_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Ridge Regression, we add a penalty so that sum of squared $\\beta$ terms does not get too large. \n",
    "\n",
    "$$ \\hat{\\beta} = \\underset{\\beta}{\\arg\\min} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where $\\lambda$ controls the amount of shrinkage. Higher values of $\\lambda$ will result in more shrinkage. This applies for more than just the linear regression problem, and is known *weight decay* in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often, we want to standardize the inputs before running regularization so that the penalty is applied evenly. We can standardize a given variable x, by\n",
    "\n",
    "$$ z_i = \\frac{x_i - \\bar{x}}{\\sigma(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "in addition, we only want to penalize the non-intercept terms. If all of the predictors have been standardized, then $\\beta_0$ is just the mean of the y values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LASSO penalty is similar to the ridge penalty:\n",
    "\n",
    "$$ \\hat{\\beta} = \\underset{\\beta}{\\arg\\min} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^{p}|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, the same $\\lambda$ parameter controls the regularization. Unlike the ridge regression, this can actually shrink some of the coefficients, $\\beta$ to 0. Therefore, unlike Ridge Regression, the LASSO also preforms *variable selection*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./assets/constraint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./assets/full.png)\n",
    "https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are other methods that take both the ridge approach as well as the LASSO approach. One such method is known as the Elastic Net.\n",
    "\n",
    "$$ \\hat{\\beta} = \\underset{\\beta}{\\arg\\min} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}x_{ij}\\beta_j)^2 + \\lambda_1\\sum_{j=1}^{p}|\\beta_j| + \\lambda_2\\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "This is often used for problems known as \"large $p$, small $n$\", where the number of predictors is quite large in comparison to the number of samples, which is common in genomics. Ridge Regression may not be able to run due to the large number of predictors, but the LASSO may select too few parameters (it will shrink too many to 0).\n",
    "\n",
    "This is a good middle ground that will select groups of correlated predictors, in theory."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
